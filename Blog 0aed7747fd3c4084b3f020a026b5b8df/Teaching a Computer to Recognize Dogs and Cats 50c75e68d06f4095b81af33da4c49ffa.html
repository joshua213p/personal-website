<html><head><link rel="shortcut icon" href="http://www.iconj.com/ico/u/1/u1bkuj55ud.ico" type="image/x-icon" /><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Teaching a Computer to Recognize Dogs and Cats</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

/* A link that is hovered on */
a:hover {
    color: rgb(20, 142, 167);
	text-decoration: underline;
}
/* A link that is selected */
a:active {
    color: #1BB0CE;
	text-decoration: underline;
}

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}

</style></head><body><article id="50c75e68-d06f-4095-b81a-f33da4c49ffa" class="page sans"><header><h1 class="page-title">Teaching a Computer to Recognize Dogs and Cats</h1></header><div class="page-body"><p id="ae42dd04-11fa-4534-8730-0e6a10e76f49" class=""><span class="user">@Joshua Payne</span> | <time>@Dec 27, 2019</time> | <mark class="highlight-gray">10 minute read</mark> | <mark class="highlight-gray"><a href = "../index.html">Home</a></mark></p><hr id="1c7234ea-d80e-4ee5-ab3c-b22bf51aeece"/><blockquote id="e91532c4-b0d7-47c2-8c8e-a79f2e0412d7" class=""><em>A/N: This article assumes a good understanding of convolutional neural network architecture. For background,¬†</em><a href="https://medium.com/datadriveninvestor/a-beginners-guide-to-convolutional-neural-networks-49384c75d1a"><em>check this out</em></a><em>.</em></blockquote><blockquote id="6b47b589-fea2-4800-b7e2-e3310ad9ee15" class=""><em>A/N: The code used in this article is based off a¬†</em><a href="https://pythonprogramming.net/convolutional-neural-network-deep-learning-python-tensorflow-keras/"><em>tutorial</em></a><em> from Sentdex. A basic understanding of Keras is encouraged.</em></blockquote><p id="6989a0e9-cc3e-45d0-b523-1ccebb024047" class="">
</p><p id="83049201-9d75-4c4d-95b3-7492beb5d9d2" class=""><u>Table of Contents</u></p><nav id="3baf5033-d45e-413c-af43-3a0b35dab8f5" class="block-color-gray table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#9536eaa4-c13f-4d4c-9910-33b2660daafe">Convolutional neural networks simplify this process.</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#65aaa032-9436-4cc5-8ca9-66069e4e9fdf">Let‚Äôs first refresh ourselves on the intuition behind CNNs.</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#7308744f-a040-4379-99f0-655d8144c9b1">Now, let‚Äôs actually build this!</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#b2a04bf6-dcf8-4f6b-ace7-115d8459fff9">Loading in and Preprocessing the Data</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#84d54f37-9f00-400e-9410-e8d21dd17fee">Building the CNN</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#e5723663-7959-4c6d-bb6e-1b26740c7028">Seeing Our Model in Action!</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#35f7699d-9ed0-4572-a151-7059f28271b8">Key Takeaways</a></div></nav><p id="176c36a8-42b1-4058-adb3-f125cee7b40a" class="">
</p><p id="39fc59ea-b594-4953-b4dc-02a1f7f1729d" class=""><strong>Are you a catüê±  or a dogüê∂  person?</strong></p><figure id="4d35a49c-b266-4d8e-a81e-d1d92e7ec234" class="image"><a href="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/Untitled.png"><img style="width:700px" src="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/Untitled.png"/></a></figure><p id="07ff1e07-b7c5-4195-89d4-8f07cd7eb183" class="">
</p><p id="f446673a-68af-4d44-b28b-81678341e9c3" class="">For some of us, we‚Äôre firmly placed on one side of this question. Cats are cute, agile, and independent furballs, while dogs are playful, personable, and defensive! <del>AÃµlÃµsÃµoÃµ,Ãµ ÃµtÃµhÃµeÃµyÃµ‚ÄôÃµrÃµeÃµ ÃµwÃµaÃµyÃµ ÃµbÃµeÃµtÃµtÃµeÃµrÃµ Ãµ</del>üòâÃµ.Ãµ</p><p id="6299c809-b1b4-4ee5-8dad-22525e78197b" class="">Nonetheless, we‚Äôre able to prefer either of these animals because, as human beings, we‚Äôve learned to differentiate between them. After years of learning what dogs and cats look like, with experience, we‚Äôve come to favor one or the other.</p><p id="cd4d404d-05c7-46db-9e92-2a1c2af1bce4" class="">
</p><p id="6154b3c9-e0c3-4abf-8922-debd1adec7cc" class="">Now, imagine replicating this process onto a computer. I tried to do so, and instantly hit roadblocks.</p><figure id="0ba877be-fc1d-4ff1-9ca1-44b37e55e048" class="image"><a href="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/Untitled%201.png"><img style="width:361px" src="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/Untitled%201.png"/></a></figure><p id="dcb0d3e3-eab5-4151-ad31-0a38cde2862f" class="">
</p><p id="a94b95d2-886d-4b57-91df-e919fd10e3c0" class="">
</p><p id="9bd8347d-d426-4b43-8f18-2b1cfc89188e" class="">The logic seems to check out, but this program is obviously not robust. How do we look for whiskers? How do we look for a tail? It‚Äôs easy for us to do, but how do we instruct a computer to scan for these features?</p><p id="b278be84-3127-4290-8536-41d249279aa9" class="">Well, for ordinary code, it‚Äôs virtually impossible to develop a program that can identify cats from unfamiliar photos with reasonable accuracy.</p><p id="e2d29c55-0368-4bda-9280-45a6c06db38e" class="">
</p><p id="d6bb7541-a3e7-4051-b421-cfa83a99238c" class="block-color-blue"><strong>So why use ordinary code? </strong></p><p id="f0540d39-193a-4f73-b417-526618ed7de7" class=""><strong><mark class="highlight-blue">With deep learning, we don‚Äôt need to.</mark></strong></p><h2 id="9536eaa4-c13f-4d4c-9910-33b2660daafe" class=""><strong>Convolutional neural networks simplify this process.</strong></h2><p id="250881c0-a2d2-4958-990e-36163a49ef86" class="">With our conventional programming example, to teach a computer to recognize features in cats, even the most bare-boned program would probably need thousands of lines of code.</p><p id="050c1b83-2fd9-4639-934e-729fb1130046" class="">With a CNN, I got this number down to¬†<strong>70</strong>.</p><p id="17b561e9-1547-4917-b45b-8f09446d5ba7" class="">
</p><h3 id="65aaa032-9436-4cc5-8ca9-66069e4e9fdf" class=""><strong>Let‚Äôs first refresh ourselves on the intuition behind CNNs.</strong></h3><p id="e45b5a73-73ba-4d7a-8cab-c60fe2112c25" class="">Humans have years of experience learning what a cat and dog look like, probably from our childhoods (throwback to picture books and Dora). In contrast, our computers haven‚Äôt had the opportunity to learn to recognize these creatures.</p><p id="ac5ee016-9db5-47c6-af72-b22d351db4a8" class="">With artificial intelligence through CNNs, we overcome this issue by¬†<em>letting them learn</em>! Our neural network learns from the data provided by our dataset. Iteratively adjusting kernel values to lower loss, our model learns how to recognize features in images. Detected features from input images help the network predict which animal the image comprises of. So, for example, if whiskers were detected in an image, our convolutional neural network would likely predict that it was looking at a cat.</p><p id="da99abe7-6d12-4082-bd43-d92ba3e19ce6" class="">
</p><figure id="dcf84f6c-feb6-472d-95ae-89b925ef7610" class="image"><a href="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/1_oB3S5yHHhvougJkPXuc8og_(1).gif"><img style="width:700px" src="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/1_oB3S5yHHhvougJkPXuc8og_(1).gif"/></a><figcaption><center>The input image is the cat, and the model accurately predicts this.</center></figcaption></figure><p id="73049249-e8aa-444a-8661-e4f507986709" class="">
</p><h2 id="7308744f-a040-4379-99f0-655d8144c9b1" class=""><strong>Now, let‚Äôs actually build this!</strong></h2><p id="2b3ae22e-74d6-4a4a-b9f2-66a10fefc5a3" class="">To begin, we‚Äôll use the¬†<a href="https://www.microsoft.com/en-us/download/details.aspx?id=54765">Cats and Dogs dataset from Kaggle</a>. I downloaded it as a folder called PetImages. It consists of 12499 photos of dogs and cats each.</p><p id="98d89276-6fd3-4fd5-8738-d350e66e10db" class="">
</p><figure id="3d73885e-66f9-44d5-b5f3-470ab6706283" class="image"><a href="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/Untitled%202.png"><img style="width:700px" src="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/Untitled%202.png"/></a><figcaption><center>The sixth dog photo in the dataset</center></figcaption></figure><p id="94270566-9481-44fd-8490-6e20d4daa33d" class="">
</p><p id="7ad39b6a-2fd5-4be6-be28-377d6d56a1d5" class="">We now need to make these images accessible to our future code (for our CNN).</p><p id="00c67cfb-59b5-406d-a3c9-d211877a9ef2" class="">
</p><h3 id="b2a04bf6-dcf8-4f6b-ace7-115d8459fff9" class=""><strong>Loading in and Preprocessing the Data</strong></h3><p id="a6f57c32-c0ae-49ce-b00c-f021a7d60f09" class="">Firstly, let‚Äôs import the following libraries.</p><pre id="ef6a293a-c178-4164-a501-056cc60b5c96" class="code"><code>import numpy as np
import os
import cv2</code></pre><p id="870e9469-c6ef-401a-b98d-9a769552a748" class="">
</p><p id="34d92db1-7c6e-44e7-9e28-18f9cf777597" class="">We‚Äôll now initialize variables to represent the file path of the dataset and the categories of our data. These categories are the sub-folders of our main folder, aka our dataset, PetImages.</p><pre id="5ed5ae84-2932-4d5a-ade3-108b3bd1200a" class="code"><code>DATADIR = ‚Äú/Users/joshua/Desktop/PetImages‚Äù
CATEGORIES = [‚ÄúDog‚Äù, ‚ÄúCat‚Äù]</code></pre><p id="69b350c4-2dd8-46eb-9d0f-c42645c99eeb" class="">
</p><p id="1310dbbb-c083-4166-bc09-08698254abb0" class="">IMG_SIZE refers to the dimensions of the photos they‚Äôll be reduced to. Our training data will be a list of the resized photos, represented in pixel values, alongside their class (cat or dog) in the form of 0 or 1.</p><pre id="205df8ec-4a8f-4e53-9d6a-325cab084988" class="code"><code>IMG_SIZE = 50
training_data = []</code></pre><p id="2808528a-3e0b-4cb1-9d86-f62fa04321b9" class="">
</p><p id="3b673acc-3281-465a-9f15-68783c855535" class="">
</p><p id="c49412f3-885c-42ef-b168-f05edb735ead" class="">Now, let‚Äôs make the function to actually load in and preprocess our data.</p><p id="379e0920-c42a-4cec-b4e7-dd643e25ab1a" class="">Our function here is called create_training_data, with a self-explanatory name. Although later on, this dataset in itself will be split into training and testing data. So technically, not all of the training data will be employed as training data.</p><pre id="df9918d1-5e02-4325-a0fc-4f6a4dc248c7" class="code"><code>def create_training_data():
    for category in CATEGORIES:
        path = os.path.join(DATADIR, category)
        class_num = CATEGORIES.index(category)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path, img),                         cv2.IMREAD_GRAYSCALE)
                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
                training_data.append([new_array, class_num])
            except Exception as e:
                pass</code></pre><p id="dd167ddf-9421-493c-ae78-a4e45de10693" class="">
</p><p id="4e5667af-3f7a-4b76-8bdb-488e46bbf7b2" class=""><strong>Let‚Äôs break down how create_training_data() works.</strong></p><pre id="599e2695-b9fa-4c13-b033-b05732250067" class="code"><code>for category in CATEGORIES:
        path = os.path.join(DATADIR, category)
        class_num = CATEGORIES.index(category)</code></pre><p id="10f7b648-94ff-4766-992f-78ff44bf05e8" class="">
</p><p id="54cf2645-69e2-474a-bfae-f400da6543aa" class="">
</p><p id="fde6c1f0-0697-4d71-92bc-12f5edc3b44a" class="">For every category, ‚ÄòDog‚Äô and ‚ÄòCat‚Äô, the variables path and class_num are defined.</p><p id="6f6133da-9eaa-4432-98a0-b06d0c90fc3d" class="">With ‚Äò<em>path = os.path.join(DATADIR, category)</em>‚Äô, the ‚Äò<em>os.path.join</em>‚Äô function combines both of the provided strings into one file directory path. In my case, ‚Äò<em>/Users/joshua/Desktop/PetImages</em>‚Äô and ‚Äò<em>Cat</em>‚Äô or ‚Äò<em>Dog</em>‚Äô are combined. This would output:</p><pre id="0e92a454-3b5a-4d2e-bb44-4d304a1d4a46" class="code"><code>/Users/joshua/Desktop/PetImages/Dog
/Users/joshua/Desktop/PetImages/Cat</code></pre><p id="81c88a47-75b4-464a-823d-4fa4fa1d7d4c" class="">
</p><p id="a0dd8f1a-0562-4fc8-ba30-3d6a33320c3e" class="">
</p><p id="208beb71-4833-4110-87c6-825ad0746520" class="">This now makes a path directly to the two sub-folders, Dog and Cat, in our PetImages dataset, so our code can access both categories of photos!</p><p id="4aa9ed8c-4186-4421-89d3-7be4bdc2bf59" class="">Now, nested in this for-loop is another for-loop. This means that this loop runs twice, once for both the data within the Dog sub-folder and the Cat sub-folder.</p><p id="df2a7a20-9754-4977-95d3-ea0b2d4e9389" class="">Our code loops for each photo inside each sub-folder. Meaning, each dog photo or each cat photo.</p><pre id="29d58237-b9fb-486f-80bd-c1050463ea33" class="code"><code>for img in os.listdir(path):</code></pre><p id="944a17e6-3845-4b1d-9db9-eee084429ce9" class="">
</p><p id="09c5352c-456a-4891-b57f-5cfb26c3c1b6" class="">
</p><p id="0a10702e-5533-457c-beb4-891009c232da" class="">For each of these photos, we¬†<em>try</em>¬†to make them an array (<em>img_array</em>) as an image in¬†<em>grayscale¬†</em>(<em>cv2.IMREAD_GRAYSCALE</em>).</p><p id="0faae4ec-ecd1-426f-9f0b-d34301e3f3b8" class="">The ‚Äò<em>os.path.join</em>‚Äô function is used again to access each piece of data in the subfolder. ‚Äò<em>/Users/joshua/Desktop/PetImages/Cat/1‚Äô¬†</em>would access the first cat photo (1.jpg).</p><p id="b0572dab-fcdc-4520-b660-021fdaeffc23" class="">The image array is then resized by the dimensions of our¬†<em>IMG_SIZE</em>¬†variable from before (<em>IMG_SIZE = 50</em>, so the photo becomes 50 by 50). This array,¬†<em>new_array</em>, is then added to our training data alongside its class number defined in our first for-loop. Remember, this class number (<em>class_num</em>) represents if the photo is a dog or a cat, 0 or 1.</p><p id="f2f6ce30-6869-452b-8832-6b584ceaea57" class="">If we encounter a file that isn‚Äôt able to be added to our training data, we ignore it (<em>pass</em>).</p><pre id="fa28f9e0-dc13-4d77-8ceb-799616552b05" class="code"><code>try:
                img_array = cv2.imread(os.path.join(path, img),                         cv2.IMREAD_GRAYSCALE)
                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
                training_data.append([new_array, class_num])
except Exception as e:
                pass</code></pre><p id="a95c010b-4373-4d14-a0d2-8d22a15aa401" class="">
</p><p id="d574bbb0-db82-4808-97f5-99c95499505f" class=""><strong>Let‚Äôs tie everything together now.</strong></p><p id="111dfbdc-3fd6-4f8f-93b5-1617d0bea1f2" class="">
</p><p id="0e603b3b-70d4-4c1a-a11c-7c609f1f1329" class="">
</p><p id="43b6561f-df22-447e-8f0a-7d55e029366b" class="">In summary, our function goes through each file in the Dog and Cat directories, and tries to add each in the form of a grayscale, resized array to our training data.</p><p id="160ad0cc-9f87-461a-a1de-b2d32582d102" class="">We can now call our function to actually create our training data, through running the code inside of it.</p><pre id="a0924308-9faa-4e15-9060-967acaafad06" class="code"><code>create_training_data()</code></pre><p id="695b04bd-e7ec-4d56-91bd-86f891a067f4" class="">
</p><p id="f315a4a2-3562-437a-924e-eb7a94a0316f" class="">
</p><p id="adbbc16b-22ca-4a71-9829-999c660b2320" class="">Now, because our function went through our PetImages chronologically, it added all of the dog photos at once to our training data, and then the cat ones. That means the¬†<strong>top half of our training data list is all dog photos, and the bottom half is all cat photos</strong>. This means that when we later try to split a part of our data into validation data, it‚Äôs largely possible that all of its photos will be either cats or dogs.</p><p id="3babfdf3-8ddd-4906-8451-376396e2236f" class="">To avoid this, we need to¬†<strong>shuffle our training data</strong>.</p><pre id="8a8511a4-6b9f-4109-ad3b-866baf07348c" class="code"><code>import random
random.shuffle(training_data)</code></pre><p id="0af0b993-367f-4c9b-b70a-702c8a5c694a" class="">
</p><p id="b1a69958-d398-4494-87be-79a72932ee29" class="">
</p><p id="3fe1bb38-6d67-4d5e-9121-b4dffee1db25" class="">Our training data needs to be separated into features and labels, as this is the basis for all supervised learning models.</p><p id="c580b603-aa70-4c29-8c37-fe4d78259754" class="">Our training data list‚Äôs items are separated by the image array and the class number. The array corresponds to the features, and the class number is the label. We sort through each¬†<em>training_data</em>¬†item and add their features and labels to X and y.</p><pre id="38405cfe-2815-4e4d-b6c7-aa740b664723" class="code"><code>X = []
y = []
for features, label in training_data:
    X.append(features)
    y.append(label)</code></pre><p id="15f6999f-4123-4bca-af7d-2ea17f1f74e1" class="">
</p><p id="e7a17ed6-03d3-4025-ad01-4ac90058854f" class="">Next, converting X and y into numpy arrays and reshaping X:</p><pre id="1d79760e-895e-4a99-9637-3328cf5d7632" class="code"><code>X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)
y = np.array(y)</code></pre><p id="60d8bea4-6862-4627-ae37-b118196f71e3" class="">
</p><p id="20e72026-f55c-4913-bd53-8c739683511d" class="">
</p><p id="f70055f0-2445-47d7-9c9d-8e6ac6772e0e" class="">To access these variables from different programs, we can save them and their data as pickle files.</p><pre id="3551decb-c077-4d08-bf38-67ed728e8b6e" class="code"><code>import pickle
pickle_out = open(‚ÄúX.pickle‚Äù, ‚Äúwb‚Äù)
pickle.dump(X, pickle_out)
pickle_out.close()
pickle_out = open(‚Äúy.pickle‚Äù, ‚Äúwb‚Äù)
pickle.dump(y, pickle_out)
pickle_out.close()</code></pre><p id="ddd5e958-036c-4778-b4f9-a4ea1c96bc37" class="">
</p><p id="da0b97c7-99d9-469f-837d-af07969e1f3a" class="">We now have our entire dataset loaded in, represented by the variables X and y for, respectively, their features and labels. These lists have been saved to our computer as pickle files (X.pickle, y.pickle).</p><p id="013c1fcf-7782-4b42-91e4-722c68d979ce" class="">
</p><h2 id="84d54f37-9f00-400e-9410-e8d21dd17fee" class="">Building the CNN</h2><p id="7c92b939-27b2-4245-9e85-d9c60fcfeda5" class="">Alright! Now that we have all of our data preprocessed and ready to go, let‚Äôs build our actual neural network!</p><p id="52745b17-743c-4fa2-a2a6-2bdb21a67c35" class="">In a new file, we‚Äôll now import the following:</p><pre id="312f3799-97d7-4418-8c65-4d4e0526bfc3" class="code"><code>import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
import pickle</code></pre><p id="d984d576-0b61-4bb1-a077-43b93de64244" class="">
</p><p id="91e38c68-c2f9-488a-8d70-7bd30341b5e9" class="">After that, to use our data, let‚Äôs load our X and y pickle files. (rb refers to reading the files)</p><pre id="bcaaaae1-3a1a-427e-8ff4-a5fa7edba2ff" class="code"><code>X = pickle.load(open(‚ÄúX.pickle‚Äù, ‚Äúrb‚Äù))
y = pickle.load(open(‚Äúy.pickle‚Äù, ‚Äúrb‚Äù))</code></pre><p id="b9e7be47-3a37-45b9-9b60-98cc0a72e840" class="">
</p><p id="18cfc4a4-f31c-42e5-85f2-a3bea099beb5" class="">For further preprocessing, we‚Äôll divide our features (pixel values) by 255 to get values solely between 0 and 1.</p><pre id="e4847110-9f8b-449f-99bf-3a03faa4410c" class="code"><code>X = X/255.0</code></pre><p id="23b61995-acd5-4581-9f7f-26dffffb88a3" class="">
</p><p id="2d60f3bd-a6e6-466c-919c-682dfba1cbf5" class="">We can now start building our neural network! We‚Äôll use the <strong>Sequential</strong> model.</p><pre id="f742b590-6abd-4aba-9c4e-af2dc080c4b8" class="code"><code>model = Sequential()</code></pre><p id="84f8247e-771b-4613-adac-2d19b1e70c04" class="">
</p><p id="f452489d-a64c-41e8-8d7c-089777827423" class="">We‚Äôll then add our first convolutional and pooling layers:</p><pre id="a6e0c3eb-c036-4a32-a1f2-bdf465f1e941" class="code"><code>model.add(Conv2D(64, (3,3), input_shape = X.shape[1:]))
model.add(Activation(‚Äúrelu‚Äù))
model.add(MaxPooling2D(pool_size = (2, 2)))</code></pre><p id="32ef3242-d4a3-4eac-8620-5bdfa5d66b73" class="">
</p><p id="9a8971e9-f201-4e9b-9f9a-7844c7b4c437" class="">
</p><p id="23b199c5-563c-4ba8-9e89-56f592297b6d" class="">We‚Äôre using 64 kernels, each with dimensions of 3 by 3. Our input is our features, X, because we teach our neural network to map them to the label, y.</p><p id="b3d68379-10f7-44eb-a987-66901a3d2837" class="">They‚Äôll then pass through a ReLU activation, and then max pooled for each 2x2 local receptive field within the feature maps.</p><p id="ef9eb872-8d46-497b-b55a-f4bb34a98cc4" class="">This will be repeated two more times because after separate model optimization it showed to lead to higher accuracy. Stay tuned for articles on optimization and neural architecture search in the future üòâ.</p><pre id="79629b4b-e295-4141-9974-b6f7da843a73" class="code"><code>for l in range(2):
    model.add(Conv2D(64 , (3,3)))
    model.add(Activation(‚Äúrelu‚Äù))
    model.add(MaxPooling2D(pool_size = (2, 2)))</code></pre><p id="36984e41-6e37-49a2-8f84-7928981ccf48" class="">
</p><p id="96370fc7-9586-46c6-82c8-627d75894182" class="">We‚Äôll now flatten our pooled maps into a single layer, which will feed into our densely connected layer. These neurons will then undergo the ReLU activation.</p><pre id="f6217ce5-30ad-4322-b5ff-18bd196fa0b9" class="code"><code>model.add(Flatten())
model.add(Dense(64))
model.add(Activation(‚Äúrelu‚Äù))</code></pre><p id="594d6a81-ca15-4f44-bc68-37ab160ad1d2" class="">
</p><p id="1e8d8d17-6ab6-453c-9380-33ae0452dd09" class="">And now, our output layer, which comprises of a final neuron! This represents the prediction of the input image being a cat or a dog.</p><pre id="4156d811-6f15-48e1-8492-efe80def0ed7" class="code"><code>model.add(Dense(1))
model.add(Activation(‚Äòsigmoid‚Äô))</code></pre><p id="d1d1565e-4c43-4ef1-bad7-99a7456b06c6" class="">
</p><p id="bcce24ab-390d-4446-9650-b62821c1d45f" class="">Let‚Äôs compile our model and fit it to our feature and label data. We use a batch size of 32 to pass 32 data samples at a time. Additionally, validation_split represents 30% of our data being split into testing data.</p><pre id="b2ce5fe1-f918-4081-bdd3-883b483bd82f" class="code"><code>model.compile(loss = ‚Äúbinary_crossentropy‚Äù, 
    optimizer = ‚Äúadam‚Äù, 
    metrics = [‚Äòaccuracy‚Äô])
model.fit(X, y, batch_size=32, epochs=10, validation_split = 0.3)</code></pre><p id="3f9c46b8-4fd2-4a20-b9e4-8835dbfa7201" class="">
</p><p id="11992e9e-09af-46b0-9d6b-501fb486b5a8" class="">And save!</p><pre id="fb3d6dd9-a135-4086-bb6e-20b4feaba94e" class="code"><code>model.save(‚Äò64x3-CNN.model‚Äô)</code></pre><p id="a7a42273-b56f-4f62-9391-7ec9421aaf69" class="">
</p><p id="cdaddd76-b806-457e-9f49-febc6d5397ef" class="">Here‚Äôs what the entire model looks like:</p><pre id="c1232da1-1ad0-4912-b536-ec2ab9879c7d" class="code"><code>import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import TensorBoard
import pickle

X = pickle.load(open(‚ÄúX.pickle‚Äù, ‚Äúrb‚Äù))
y = pickle.load(open(‚Äúy.pickle‚Äù, ‚Äúrb‚Äù))
X = X/255.0

model = Sequential()
model.add(Conv2D(64, (3,3), input_shape = X.shape[1:]))
model.add(Activation(‚Äúrelu‚Äù))
model.add(MaxPooling2D(pool_size = (2, 2)))
for l in range(2):
 model.add(Conv2D(64 , (3,3)))
 model.add(Activation(‚Äúrelu‚Äù))
 model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Flatten())
model.add(Dense(64))
model.add(Activation(‚Äúrelu‚Äù))
model.add(Dense(1))
model.add(Activation(‚Äòsigmoid‚Äô)) # softmax??

model.compile(loss = ‚Äúbinary_crossentropy‚Äù, 
 optimizer = ‚Äúadam‚Äù, 
 metrics = [‚Äòaccuracy‚Äô])
model.fit(X, y, batch_size=32, epochs=10, validation_split = 0.3)

model.save(‚Äò64x3-CNN.model‚Äô)</code></pre><p id="d7f5e482-3dc0-44d5-a002-e311d455371f" class="">
</p><p id="7253563f-7980-4686-bbad-6d3d2957fae1" class="">And‚Ä¶ run! After 10 epochs, here were my final metrics.</p><pre id="30d49229-cb92-4a24-a999-9c0653479695" class="code"><code>loss: 0.2443 ‚Äî accuracy: 0.8954 ‚Äî val_loss: 0.4773 ‚Äî val_accuracy: 0.8051</code></pre><p id="f678e51c-9db8-484b-9f8a-3fdc44bd34d0" class="">
</p><p id="3fa76ad5-5540-40ef-819e-3eb1d5e9f90e" class="">
</p><h2 id="e5723663-7959-4c6d-bb6e-1b26740c7028" class=""><strong>Seeing Our Model in Action!</strong></h2><p id="c8a71221-cc8d-4cca-b0e5-57efc04073e1" class="">We‚Äôve made our CNN, but we haven‚Äôt seen it predict anything yet. Let‚Äôs do that now, and see our model in practice!</p><p id="44ad4737-c0c1-4d92-b885-4e3b254afd15" class="">Firstly, we‚Äôll need a photo of a dog our model hasn‚Äôt seen before. Let‚Äôs use this.</p><p id="69c0fb24-0983-496b-9df0-c61417042a04" class="">
</p><figure id="7ed78246-7963-4ea6-9f8f-9001db8ab77e" class="image"><a href="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/Untitled%203.png"><img style="width:300px" src="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/Untitled%203.png"/></a></figure><p id="744516e0-7643-43e8-886c-01bfdc89c86b" class="">
</p><p id="b2b8e7d5-6a12-4262-a487-5f540a6354b7" class="">We‚Äôll first import these libraries:</p><pre id="686875f1-e9ae-4234-80ff-1c3d72fd0ac5" class="code"><code>import cv2
import tensorflow as tf</code></pre><p id="1d8fece7-ff17-4d14-853d-78463665beae" class="">
</p><p id="e1742a57-7517-44b7-a3be-ba4cfe0a69ac" class="">Our model‚Äôs prediction is represented as 0 or 1, so we‚Äôll create this variable to later help us convert this prediction number into either ‚ÄòCat‚Äô or ‚ÄòDog‚Äô.</p><pre id="6874508c-1d3d-41bb-9828-8c1cd59e6a5b" class="code"><code>CATEGORIES = [&quot;Dog&quot;, &quot;Cat&quot;]</code></pre><p id="b0214e04-4c30-46b3-ae43-f7f3ca4dbd72" class="">
</p><p id="09c76569-6a60-4ae5-b8bc-61f9be6d6ec0" class="">This function will preprocess our image so it‚Äôs suitable for our ConvNet.</p><pre id="ca80eb5b-980d-43ee-bec2-f6112e63454a" class="code"><code>def prepare(filepath):
    IMG_SIZE = 50
    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) 
    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)</code></pre><p id="62e8b2cc-7ee1-405d-ad77-debe53bb8ab7" class="">
</p><p id="246afa3d-3d09-4dfb-bbe4-181ce34ae99c" class="">We‚Äôll now load in our model, saved when we trained it previously:</p><pre id="41e07995-ee2a-4290-852e-de52472d1824" class="code"><code>model = tf.keras.models.load_model(‚Äú64x3-CNN.model‚Äù)</code></pre><p id="525d0d93-8853-47b6-9d80-e2a5a3d5fdc4" class="">
</p><p id="0e11e9d9-5625-4c04-8052-96cf88d7ec2d" class="">Let‚Äôs now actually use our model to predict what the image is! We‚Äôll use our prepare function on our photo, and our model will predict what animal it is in the form of 0 or 1. This value will be stored in our prediction variable.</p><pre id="20409d50-c81f-494d-abb0-57b5f5148b9c" class="code"><code>prediction = model.predict([prepare(&#x27;dog.jpg&#x27;)])</code></pre><p id="886a6d3e-2df6-4298-818f-a5ad974a256f" class="">
</p><p id="7c41e7dd-6a44-4e4e-9c87-df9ece93f8bd" class="">Our prediction variable is a two-dimensional array, and so to actually get a 0 or 1 value, we‚Äôd use this.</p><pre id="9826bd3d-1639-486a-a4f7-b7010deb899d" class="code"><code>prediction[0][0]</code></pre><p id="cc405312-bfe9-4583-b046-08b167277f08" class="">
</p><p id="3671db01-0b8c-4384-bedd-ea2d2eaf5310" class="">This outputs 0, but it doesn‚Äôt tell us whether the photo is a cat or a dog directly. Let‚Äôs fix this using our ‚ÄòCATEGORIES‚Äô variable from before. We‚Äôre telling our code to print the item in the list at index 0.</p><pre id="2f2281ca-238e-45b8-b3f7-9b69a4fbd1bf" class="code"><code>print(CATEGORIES[int(prediction[0][0])])</code></pre><p id="d740bf1f-1502-45ae-bf87-0382475ccfa4" class="">
</p><p id="3724b5b2-ccd7-4411-8142-615ebfd605ca" class="">This outputs ‚ÄòDog‚Äô! Meaning our code successfully recognized dog.jpg!</p><p id="3c56ceec-f936-46ce-8b8e-f89b298ad71c" class="">
</p><p id="ead29c7e-fe14-4f53-a614-94fbc9d72dad" class="">And that‚Äôs it! That‚Äôs all the code we need to harness the power of AI. We‚Äôve now taught a computer to recognize cats and dogs from foreign photos with 80% accuracy in 70 lines of code. Despite seeming impossible at the beginning of the article, we‚Äôve managed to do it either way!</p><p id="6a2f8cd2-b733-4086-a86d-b4b8d9493071" class="">
</p><p id="7d6733bc-10bb-412d-8b9c-322a02d2ec41" class="">But this isn‚Äôt nearly the be-all-end-all for CNN application ‚Äî imagine how this could be scaled for¬†other tasks! Recognizing criminals from surveillance footage, detecting abnormalities in organs, the list truly goes on.</p><figure id="3d659b23-c9d1-479d-ae3e-f03701ed952e" class="image"><a href="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/Untitled%204.png"><img style="width:700px" src="Teaching%20a%20Computer%20to%20Recognize%20Dogs%20and%20Cats%2050c75e68d06f4095b81af33da4c49ffa/Untitled%204.png"/></a><figcaption><center>CNNs for surveillance footage - for better or for worse</center></figcaption></figure><p id="82a4ae37-938e-4ec2-9bb8-292d68699591" class="">
</p><p id="60fe2faa-d74b-4742-98c5-902869f3a9c3" class="">By enabling computers to recognize and classify images, we‚Äôve opened the door to a boundless number of solutions to real-world issues. Humans are only beginning to find areas to leverage convolutional neural networks ‚Äî where else is this possible?</p><p id="5926ae9e-6988-4069-914d-2d7b94f66819" class="">
</p><hr id="698961d3-9cdb-4c67-aac8-6edb0dee26d7"/><h3 id="35f7699d-9ed0-4572-a151-7059f28271b8" class=""><strong>Key Takeaways</strong></h3><ul id="f62db5c0-b391-4f3e-9e24-85b028b8f59b" class="bulleted-list"><li>CNN simplifies and enables digital image classification</li></ul><ul id="482220af-157a-4f1f-9e12-5ddfcb87e566" class="bulleted-list"><li>By training our model on the Cats and Dogs dataset, we‚Äôve taught it how to differentiate between the two</li></ul><ul id="de8b7611-5a88-4dc9-a6ab-058172bf042b" class="bulleted-list"><li>Tensorflow and Keras allow for concise neural net programming</li></ul><ul id="a6ca1f81-fb17-40b9-8263-9e85ff7f45b1" class="bulleted-list"><li>We achieved 80% validation accuracy!</li></ul></div></article></body></html>